\chapter{Changes to Vortex} \label{chap:changes}

In this chapter I will first give an overview of the \Gls{vortex} architecture and pipeline. Then I will describe the identified bottlenecks and the proposed changes to improve scheduling and frontend throughput. 

\section{Vortex Architecture}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/vortex-pipeline2-no-background.png}
    \caption{\Gls{vortex} \Gls{riscv} 5-stage pipeline of a \acrfull{sm}}
    \label{fig:vortex_microarchitecture}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/vortex-cluster-color.png}
    \caption[Clustering of \acrshortpl{sm} in \Gls{vortex}.]{Example of \acrshortpl{sm} clustering in \Gls{vortex} using 2 clusters and 4 \acrshortpl{sm} per cluster. The L2 and L3 caches are optional, if they are not included, memory arbiters are used.}
    \label{fig:clustering}
\end{figure}

The \Gls{vortex} microarchitecture is illustrated in Figure \ref{fig:vortex_microarchitecture}. Each \acrshort{sm} is a 5-stage pipeline containing a \textit{fetch}, \textit{decode}, \textit{issue}, \textit{execute} and \textit{commit} stage. The \acrshortpl{sm} of \Gls{vortex} all have their own L1 and instruction caches. There are also options for \Gls{vortex} to include L2 and L3 caches, which are shared among \acrshortpl{sm}, as shown in Figure \ref{fig:clustering}. If the L2 or L3 caches are not included, they are replaced by memory arbiters. \Gls{vortex}' pipeline is elastic\cite{elastic_pipeline}, i.e. it is using \textit{ready} and \textit{valid} signals to communicate between the modules and pipeline stages. This makes it easier to add new or configure existing components, as the modules can be more flexible in the number of cycles they use.

\subsection{pipeline} \label{sec:vortex_pipeline}
Following is a description of the five pipeline stages of \Gls{vortex}' \acrshortpl{sm}.

\vspace{1mm}\noindent
\textbf{The fetch stage} is responsible for scheduling warps, keeping track of branches, divergence and barriers. The warp scheduler keeps track of the active, stalled and blocked warps using bitmasks. The baseline version of \Gls{vortex} use a \textit{find-first} scheduler, which prioritizes scheduling the ready warp with the lowest warp ID. This will be further explained in Section \ref{sec:warp_issue_scheduler}. Upon scheduling a warp, the warp scheduler sends an instruction-fetch-request to the icache-stage in decode. In the baseline version of \Gls{vortex}, only one warp from each \acrshort{tb} can be fetched concurrently. Since vortex does not support branch prediction, the warp has to be stalled until it is known that it cannot change control-flow. The first point in the pipeline this can be known is after decoding the instruction. If the instruction is not a branch, barrier or thread control, the warp will no longer be marked as stalled in the warp-scheduler. Otherwise, the warp continues to be stalled until the instruction completes execution.

\noindent
\textbf{The decode stage} contains a decoder and the icache-stage. The icache stage is illustrated in Figure \ref{fig:baseline_icache_stage}. The purpose of the icache-stage is to enable fetching instructions from multiple \acrshortpl{tb} concurrently. The icache-stage use dual-port RAM with one write address per \acrshort{tb} in the \acrshort{sm}, to store information about the requested instruction. The request is simultaneously sent to the icache. Upon receiving a response from the icache, the corresponding request information is read from RAM and combined with the instruction data, to create the response. The response is then sent to the decoder, which decodes the instruction using sequential logic. The decoder also informs the fetch stage whether or not the instruction can change control flow. After decoding, the instruction is sent to the instruction buffer in the issue stage.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Baseline Icache-stage.png}
    \caption{Illustration of \Gls{vortex}' icache-stage.}
    \label{fig:baseline_icache_stage}
\end{figure}

\vspace{1mm}\noindent
\textbf{The issue stage}, shown in Figure \ref{fig:baseline_issue_stage}, is responsible for issuing the warps to the functional units. To handle data dependencies, \Gls{vortex} utilize scoreboarding. The warps are issued in order, but is committed in the order in which the execution is completed. When instructions are decoded, they are transferred to the instruction buffer (ibuffer). The ibuffer contains a queue for each \acrshort{tb} in the \acrshort{sm}. The issue stage has an instruction scheduler which schedules warps from the front of one of the queues in the ibuffer and dispatches it to the corresponding functional unit in the execution stage.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/Baseline Issue Stage.png}
    \caption{Illustration of \Gls{vortex}' issue stage}
    \label{fig:baseline_issue_stage}
\end{figure}

The instruction scheduler used in the baseline version of \Gls{vortex} is a round-robin scheduler. For a warp to be issued, the source and destination registers must be available in the scoreboard and the corresponding functional unit must be available in dispatch. The instruction scheduler attempts to issue one of the available warps in the ibuffer in a round-robin order. The issue stage can however in each cycle only check for one warp whether it is ready or not. Thus if the selected warp cannot be issued, no warp is issued in that cycle. I cover this problem in greater detail and propose an improvement in Section \ref{sec:ready_scheduling}. 
% \noindent
% Observe that if the queue in the Ibuffer, corresponding to the warp id of the warp in decode is full, the front-end will stall. Thus if one warp is stalling, other warps can be blocked from scheduling.

\vspace{1mm}\noindent
\textbf{The execute stage} consists of five functional units. The \textit{\acrlong{alu}} (\acrshort{alu}) performs logic and integer arithmetic operations in addition to handling branches. The \textit{\acrlong{lsu}} (\acrshort{lsu}) performs memory loads and stores. The \textit{\acrlong{csr}} (\acrshort{csr}) holds a number of status registers which can be read and written to using csr instructions. Some of these influence how graphics are rendered, by controlling sampling modes, texture addresses etc. The \acrshort{csr} also track a number of performance metrics, such as the number of commited instructions and the number cycles used. The \textit{\acrlong{gpu}} (\acrshort{gpu}) performs texture sampling as well as sending control signals to the fetch stage in regards to warp spawning, divergence, barriers and thread masks. Lastly the \textit{\acrlong{fpu}} (\acrshort{fpu}) performs floating point operations.

\vspace{1mm}\noindent
\textbf{The commit stage} is the last stage in the pipeline. When instructions are finished in the execute stage, the results are written back to the \acrfull{gpr}. The registers reserverd by the instruction is also released in the scoreboard.

\subsection{Workload Distribution}

\Gls{vortex}' workload distribution and \acrshort{tb} scheduling is performed statically. Upon starting the execution of a kernel, each \acrshort{sm} calculates the total number of active \acrshortpl{sm} required. The number of \acrshortpl{sm} required is calculated as:
\begin{equation}
    \#SMs_{active} = \text{min}(\frac{\#WorkGroups}{\#WarpsPerSM \times \#ThreadsPerSM}, \#SMs)
\end{equation}
\noindent
The work-groups are then divided evenly among the active \acrshortpl{sm}. If there are more work-groups than the total number of threads available in \Gls{vortex}, the last \acrshort{sm} is allocated all of the remaining work-groups. This can result in very inefficient workload distribution, and the programs running on the \acrshort{gpu} have to be adjusted before compilation to best fit the architecture. Static workload distribution also have another weakness. If for some reason the \acrshortpl{sm} require different amount of time to complete their workload, the \acrshortpl{sm} which finish early, end up idling while waiting for the other \acrshortpl{sm} to complete their workloads. The difference in execution time, can stem from multiple causes. The memory system could prioritize requests from certain \acrshortpl{sm}, or the workloads could differ, for example in terms of divergence. This results in unused computational power and reduces throughput of the \acrshort{gpu}. While I do not propose or implement any solutions to this problem in this thesis, it is relevant for understanding the results and evaluation in Chapter~\ref{chap:results}.

\section{Scheduling Algorithms}

\subsection{Ready Scheduling} \label{sec:ready_scheduling}

The baseline issue scheduler in \Gls{vortex}, described in Section \ref{sec:vortex_pipeline}, attempts to issue an available warp before checking if it is ready. As the scheduler is round-robin, this can potentially be detrimental to the performance. If 1 out of $N$ warps are ready while the $N-1$ other warps are stalling, the scheduler could potentially waste $N$ cycles before issuing the warp. An example of this is shown in Figure~\ref{fig:unready_lrr}. While this is the most extreme case, cases of similar severity are likely to occur. Using a \textit{ready scheduler}, which checks for ready warps before scheduling, will guarantee that it only takes 1 cycle to schedule a warp if at least one is ready.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/old_lrr_scheduler.png}
    \caption{Illustrating the inefficiency of the baseline issue scheduler. The \textit{unready} \acrshort{lrr} scheduler wastes cycles by attempting to issue stalled warps from the ibuffer}
    \label{fig:unready_lrr}
\end{figure}

Figure \ref{fig:new_issue_stage} illustrates the new issue stage implemented by me. It performs the ready check for all warps before the instruction scheduler selects the warp to issue. Each warp checks if it is ready by: \textcircled{\small{1}} Check in dispatch if the corresponding \acrshort{fu} is available. \textcircled{\small{2}} Check in scoreboard if the required operands are available. \textcircled{\small{3}} Use bitwise AND to create a bitmask of ready warps. \textcircled{\small{4}} Lastly, create a bitmask of valid warps which can be issued, by finding the bitwise AND of the ready warps and the warps available in the ibuffer. These changes result in a ready \acrfull{lrr} scheduler as described in Section~\ref{sec:warp_scheduling}.

The implementation of the new issue stage does not require much additional hardware. All the information is already available in scoreboard and dispatch, thus it mainly requires selectors for reading the correct registers. The scheduler does not need to be changed, as it continues to selects a warp from the bitmask. 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/new_issue_stage.pdf}
    \caption[Illustration of the new issue stage.]{Illustration of the new issue stage allowing the instruction scheduler to check for ready warps before selecting which warp to issue.}
    \label{fig:new_issue_stage}
\end{figure}


\subsection{Greedy then Oldest}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/gto_numbers.png}
    \caption{Implementation of \acrfull{gto}.}
    \label{fig:gto_impl}
\end{figure}

After implementing ready scheduling, the possibility of implementing other scheduling algorithms become available.\Acrfull{gto} is a common and easy to understand algorithm with a potential to give improved performance over \acrfull{lrr}. I was unable to find implementation details of existing \acrshort{gto} schedulers, thus I had to implement my own. Figure~\ref{fig:gto_impl} illustrates my implementation of \acrshort{gto}. \textcircled{\small{1}} Is the set of ready warps in the form of a bitmask. \textcircled{\small{2}} Contains the ages of the warps, i.e the number of valid schedules which occurred, while the warp was ready. The age is calculated as described by Equation \ref{eq:age}. Issuing a warp might cause structural stalls for the other warps. Thus an alternative would be to use the warps' presence in the instruction buffer as a condition for incrementing the age, rather than it being ready. \textcircled{\small{3}} Use combinatorial logic to find the oldest ready warp using the ready mask and the age of each warp. \textcircled{\small{4}} Is the warp ID of the previously scheduled warp. If the previously scheduled warp ID is still ready, the greedy selector \textcircled{\small{5}} continues to select the warp ID, otherwise it selects the oldest ready warp.

% By only incrementing the age upon a schedule, the age registers does not require many bits to track the age of the warps. In my case, I found that  than if the age was incremented every cycle.

\begin{equation} \label{eq:age}
    \text{age}_i = 
    \begin{dcases}
            \text{age}_i, & \text{if } \text{valid} = 0 \text{ or ready}_i = 0  \\
            0, & \text{if } \text{valid = 1 and Scheduled WID = } i \\
            \text{age}_i + 1, & \text{otherwise}
    \end{dcases}
\end{equation}

\subsection{Matching Warp and Issue Schedulers} \label{sec:warp_issue_scheduler}

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{figures/find-first-scheduling.png}
    \caption[Demonstration of \Gls{vortex}' find-first warp scheduler.]{Demonstration of how \Gls{vortex}' find-first warp scheduler schedules warps to fetch. When the stall latency is shorter than $\text{\#Warps} - 1$ cycles, some warps are scheduled substantially less than the others.}
    \label{fig:find_first_scheduling}
\end{figure}

As explained in Section \ref{sec:vortex_pipeline}, \Gls{vortex} two schedulers, the \textit{warp scheduler} in the fetch stage, and the \textit{issue scheduler} in the issue stage. The interaction between the two schedulers might be important for the issue scheduler to achieve its goals. For example if the warp scheduler fetches warps in \acrshort{lrr} order, a \acrshort{gto} issue scheduler will not have enough warps from the same \acrshort{tb} to see the effects of greedy scheduling. It is thus desirable to use scheduling algorithms for the warp scheduler which supports the goals of the issue scheduler. To achieve this I implemented the same \acrshort{lrr} and \acrshort{gto} algorithms in the warp scheduler.

Implementing the algorithms in the warp scheduler is quite simple as the scheduler modules can be reused. Instead of using the bitmask of warps which are ready to issue as input, the bitmask of ready warps in fetch can be used for the warp scheduler. An issue which arise when implementing \acrshort{gto} in the warp scheduler is that \acrshort{gto} wants to fetch warps from the same \acrshort{tb} multiple cycles in a row. As explained in Section~\ref{sec:vortex_pipeline}, the warps are stalled after being scheduled to avoid potential control flow hazards. Thus a \acrshort{gto} warp scheduler will not have the intended behaviour when integrated into the existing pipeline. Section~\ref{sec:no_stall_scheduling} describes changes to the frontend which solve this issue.

\section{Frontend}

Previous investigations of \Gls{vortex}\cite{Aurud_Project} show that the frontend is unable to fetch enough instructions to the issue stage, impeding the GPUs ability to hide stalls. This section describes a set of changes done to improve the throughput of the frontend.

\subsection{No Stall Scheduling} \label{sec:no_stall_scheduling}

The root of the frontend problem is that \acrshortpl{tb} are stalled whenever their warps are scheduled. Most of the instructions does not require stalling and are un-stalled after being decoded. As the baseline version of \Gls{vortex} has 4 \acrshortpl{tb} per \acrshort{sm}, it is unable to hide the latency of the instruction fetch. The high number of stalls thus reduce the throughput of the frontend. To avoid this, I propose \acrfull{nss}, allowing the frontend to schedule warps without stalling, and instead flush the frontend if a stall is required. To further improve the efficiency of \acrshort{nss}, I propose \textit{stall-prediction} to reduce the number of flushed instructions, and make the frontend stall only when required.

To implement \acrshort{nss}, two mechanisms are required. 1. The icache-stage needs to handle multiple concurrent instruction fetches from the same \acrshort{tb}. The responses to these requests also have to be reordered, as the icache is non-blocking. 2. Flushing the instructions in case of a misprediction. Figure \ref{fig:new_icache_stage} illustrates my design of the new icache-stage. Instead of using dual-port RAM, allowing only one concurrent fetch request per \acrshort{tb}, the new design use a queue allowing for 8 concurrent instruction fetches per \acrshort{tb}. It is possible to implement an even larger queue, but I found that the number of in-flight requests per \acrshort{tb} rearly exceeded 8. The insert queue has two ways of writing data to it. \textcircled{\small{1}} First the fetch request is pushed into the queue and sent to the \acrshort{icache}. By using a queue, the order of the requests are maintained. \textcircled{\small{2}} Secondly, the data from the instruction cache is inserted into the queue. The UUID of the request is used to match the instruction data with the request. When the first element in the insert queue is ready, i.e it contains both a valid request and its data, it becomes available for a scheduler \textcircled{\small{3}} to forward it to decode. A scheduler is required, as it is possible for multiple queue elements to be ready at the same time. This is due to the reordering of the instruction data. Upon decoding the instruction, the warp is popped out of the queue and sent to the issue stage.

\textcolor{red}{As this is not a common occurrence, the scheduling algorithm is not likely to affect performance much. I elected to use \acrshort{gto} as it will conform with both a \acrshort{gto} and \acrshort{lrr} issue scheduler.}

To not stall, the warp scheduler have to predict what instruction to fetch next. I elected to predict that branches were never taken and the thread mask would not change. This scheme has low overhead, as the scheduling continues as if nothing changed. Upon decoding an instruction it might require the front-end to stall or the thread mask to change. When this happens all predicted requests belonging to the same \acrshort{tb} are flushed. This is done in the insert queue by setting the valid bit of each entry low. While the requests are flushed from the insert queue, the requests continue to be processed by the instruction cache. When data from flushed requests are returned, there will not be any valid matches in the queue, and the data will be ignored. 

Upon flushing, the warp is marked as stalling by the warp scheduler, the \acrshort{pc} is also reset to the instruction after the instruction causing the front-end stall. This is done by sending the \acrshort{pc} of the stalling warp back to the warp scheduler. Using the new icache-stage allows us to control the order in which the warps are scheduled to a greater degree. This is because the warps now only stall when required. 

% \textcolor{red}{This behaviour may lead to cache pollution} 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/New Icache-stage.png}
    \caption[Illustration of the new icache-stage.]{New icache-stage allowing for multiple concurrent instruction fetch requests from each \acrshort{tb}}
    \label{fig:new_icache_stage}
\end{figure}

\subsection{Stall Prediction}

The implementation of \acrshort{nss} presented in Section \ref{sec:no_stall_scheduling} has a weakness, the cost of mispredicting is large. The cost of mispredicting is high because it is highly likely that the GPU could fetch other non-speculative warps. Thus the cost is not relative to not fetching any warps, but rather relative to scheduling a number of other warps. To combat this issue I propose implementing a stall table, allowing the warp scheduler to learn if an instruction will cause a frontend stall.

An illustration of the implementation of the stall-table is shown in Figure \ref{fig:stall_table}. \textcircled{\small{1}} The \acrshort{pc} of the fetched warp is used to index the stall table. It is split into a number of tag and index bits, the number of bits is dependant on the size of the table, see Equation \ref{eq:table_size}. \textcircled{\small{2}} If the tags are matching and the table entry is valid, the instruction will require the next warp to stall. The \acrshort{tb} can then be stalled after scheduling the instruction, instead of continuing to fetch instructions which would be flushed.

\begin{equation} \label{eq:table_size}
\begin{gathered}
    \#IndexBits = \ceil*{\text{log}_2(TableSize)} \\
    \#TagBits = \#PCBits - \floor*{\text{log}_2(TableSize)} - 2
\end{gathered}
\end{equation}

For the branch table to learn, the \acrshort{pc} and stall status of the instruction is sent from decode \textcircled{\small{3}}. If the instruction is a stall, the tag and valid bit is set at the corresponding index. Note that this may invalidate a previous entry by overwriting the tag. If the instruction does not cause a stall, nothing is written to the stall table. Thus the stall table contains only the instructions which will cause a stall.

The index bits can be any subset of the bits, but I elected to use the least-significant bits, except the last two which are always zero. By using the LSB as index, the stall table will handle consecutive stalling instructions. The stall-table will only be effective when instructions are executed multiple times, i.e loops. Thus it is more valuable for the stall table to have high accuracy within the loop, than wide coverage. Some preliminary testing showed that having 128 entries gave the best result. Increasing the number of entries further did not improve much. 

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/stall_table.png}
    \caption[Illustration of the stall table.]{\textit{Stall table} used to check if an instruction is known to block the next instruction from being fetched or cause a flush.}
    \label{fig:stall_table}
\end{figure}

% \textcolor{red}{We would like to have more control over the scheduling algorithm in the warp scheduler. (explain why we cannot control the warp scheduler and thus not the issue scheduler). To do this we have to alter the way instructions are fetched}

% \textcolor{red}{icache responses can return in a different order than they were requested. They have to be reordered using insert queue. Keeps track of request order and outputs once the first element is fetched. Needs a new selection logic as it is possible for multiple elements to ready. Use GTO because it will follow Round robin. could also use Round robin but it would likely not fit with GTO in issue}

% \textcolor{red}{To reduce the number of instructions flushed due to scheduling instructions which have to stall, I implemented a stall buffer}

% \subsection{Warp scheduling}

% \textcolor{red}{After implementing no-stall-scheduling, we are able to implement a GTO in the warp scheduler}

\subsection{Back Pressure Reduction}

When implementing \acrshort{nss} and the stall table, the throughput of the front-end will increase. The increase in throughput may cause the instruction buffer to become full, resulting in back pressure as illustrated in Figure \ref{fig:backpressure}. This occurs when the the decoder decodes an instruction with a warp ID of which instruction buffer is full. As the decode is unable to send the instruction to issue, it has to stall until there is space in the ibuffer. As also show in Figure \ref{fig:backpressure}, this may be problematic because there can be instructions which can be decoded which is blocked because the ibuffer of another \acrshort{tb} is full. This will impair the throughput, as it is probable that these warps can be issued.

This problem can be resolved by implementing what I will refer to as \acrfull{bpr}. \acrshort{bpr} is quite simple a bitmask sent from the ibuffer to the icache-stage indicating which ibuffer lanes are full. This allows the icache-stage to always select instructions to be decoded which will not cause back pressure.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/backpressure.png}
    \caption[Illustration of back pressure from the ibuffer to the icache-stage.]{Back pressure in the front-end caused by the decoder not being able to output the decoded instruction because the corresponding ibuffer is full.}
    \label{fig:backpressure}
\end{figure}

% \textcolor{red}{If one of the ibuffers is full, it may cause backpressure into decode and fetch. Even if the icache stage has ready instructions, they cannot be decoded and issued. This may result in a state where only one warp is available in the ibuffer, while decode and the icache stage is stalling because it is waiting for the correct ibuffer slot to be available. If this warp is stalled due to a long latency stall, no other warps can be issued to hide the stall}

% \textcolor{red}{Figure \ref{fig:backpressure} shows the number of cycles per instruction where a warp is blocked in decode because of full ibuffers. ** Describe Notable elements of the graph **}

% \textcolor{red}{To resolve this issue, a signal is sent from the ibuffer to the warp scheduler and (icache stage for selector) indicating for each ibuffer if it is full or not}

% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{figures/backpressure/L2.png}
%     \caption{Backpressure, placeholder}
%     \label{fig:backpressure}
% \end{figure}