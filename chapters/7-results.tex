\chapter{Results and Evaluation} \label{chap:results}

% In the following sections I will present the results of the changes described in Chapter \ref{chap:changes}. The following list of configurations were tested.

% \begin{itemize}
%     \item \textbf{Vortex} is the baseline version of \Gls{vortex} as presented at MICRO'21 \cite{vortex}.
%     \item \textbf{RI} implements the new issue stage using ready-round-robin scheduling.
%     \item \textbf{GTO} Use GTO in warp scheduler and instruction scheduler. Both BPR and no-stall-scheduling (NSS) is enabled.
%     \item \textbf{RR} Use round-robin and ready-round-robin in the warp scheduler and instruction scheduler respectively. Both BPR and no-stall-scheduling (NSS) is enabled.    
% \end{itemize}

This chapter presents the results and evaluation of the experimental setup presented in Chapter \ref{chap:exp_setup}. In the evaluation, I will mainly consider the following configurations:

\begin{itemize}
    \item \textbf{Vortex} is the baseline version of \Gls{vortex} as presented at MICRO'21 \cite{vortex} (Github link)
    \item \textbf{FGTO} is the configuration implementing all the frontend changes, i.e. \acrshort{bpr}, \acrshort{nss} and stall prediction. The \acrshort{gto} scheduling algorithm is used for both the warp and instruction scheduler.
    \item \textbf{FRRR} is the configuration implementing all the frontend changes, i.e. \acrshort{bpr}, \acrshort{nss} and stall prediction. The ready version of the \acrshort{lrr} scheduling algorithm is used for both the warp scheduler and instruction scheduler.
\end{itemize}

\section{CPI Stack Overview}

\begin{figure}
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.3\textwidth]{figures/cpi_norm/L2.png}}
    \caption[Normalized \acrshort{cpi} stacks before and after the changes.]{Normalized \acrshort{cpi} stacks for the \Gls{vortex}, and the new \textit{GTO} and \textit{RRR} configurations}
    \label{fig:norm_cpi}
\end{figure}

Figure \ref{fig:norm_cpi} shows the \acrshort{cpi} stacks for all of the benchmarks and the average \acrshort{cpi}, normalized to the baseline version. The changes to \Gls{vortex} has a varying effect on the performance of each benchmark. In this context, performance is characterized by \acrshort{cpi}, where lower \acrshort{cpi} equates to higher performance. On average the \textit{FGTO} and \textit{FRRR}  configurations respectively have a $5.03\%$ and $5.48\%$ reduction in \acrshort{cpi} compared to \textit{\Gls{vortex}}. There are however large deviations from this average. The \textit{HW} benchmark see an almost a $40\%$ reduction in \acrshort{cpi} for both \textit{FGTO} and \textit{FRRR}, while \textit{CFD} see over $20\%$ increase in \acrshort{cpi} for the \textit{FGTO} configuration. Most of the benchmarks see a drastic shift in the causes of the stalls. This shift does however not necessarily reflect in an increase or decrease in \acrshort{cpi}. In the following sections I will present subsets of the results shown in Figure \ref{fig:norm_cpi}, to explain the effects of the implemented changes.

\newpage
\section{Reduction of Control Stalls} \label{sec:result_control_stalls}

The number of frontend stalls per instruction, shown in Figure \ref{fig:norm_cpi_frontend}, makes it clear that \Gls{vortex}' frontend is the dominating cause of stalls in the baseline version. On average, 50\% of the \acrshort{cpi} are stalls due to \textit{sync \& control} and/or \textit{empty ibuffer}. Of these, \textit{sync \& control} stalls are most prevalent, representing over 40\% of the \acrshort{cpi}. The new frontend implementing \acrshort{nss}, and stall-prediction reduces the number of control stalls by removing unnecessary stalls. Thus warps are now only blocked from being fetched after fetching instructions capable of changing control flow or thread mask. \acrshort{nss} in combination with the new icache-stage, allows for doing multiple concurrent instruction fetches, increasing fetch bandwidth. After implementing the new frontend, $<5\%$ of the average \acrshort{cpi} is \textit{sync \& control} stalls. This shows that most instructions were unnecessary blocking the frontend. 

% 0.0002%
For \textit{sfilter}, the number of frontend stalls are reduced from $61\%$ to $<0.01\%$ of the \textit{\Gls{vortex}} \acrshort{cpi}. This is because sfilter does not have any branch instructions in its kernel, thus the scheduler, which assumes the instruction will not stall, can continue to fetch instructions. The few occurring frontend stalls are during the end of the benchmark, when the \acrshortpl{sm} are turned off. As the instructions are never blocked in the warp scheduler, the \acrshort{gto} warp scheduler would never switch which \acrshort{tb} it schedules from. However, \acrshort{bpr} informs the warp scheduler when the ibuffer is full, blocking the warp from being scheduled. \acrshort{bpr} thus forces the \acrshort{gto} warp scheduler to switch if the backend is stalling, making it schedule more fair. This become apparent as there are no idle stalls caused by \acrshortpl{tb} completing before others. This demonstrates how the new frontend is able to remove all unnecessary control stalls.

While the new frontend has a large impact on most benchmarks, \textit{BFS} seems unable to utilize \acrshort{nss}. The number of \textit{sync \& control} stalls does not change between \textit{\Gls{vortex}}, \textit{FGTO} and \textit{FRRR}. \textit{BFS}' kernel contains a large proportion of instructions pertaining to control flow and handling of divergence. Because of this, \textit{BFS} has a considerable number of required frontend stalls, making the \acrshort{nss} implementation stall the frontend. By using stall prediction, it ensures that these control stalls are not mispredicted, i.e. to not stall, making sure that the new frontend will perform just as well as the baseline. 

\begin{figure}
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.3\textwidth]{figures/cpi_norm/cpi_frontend.png}}
    \caption[Normalized \acrshort{cpi} attributed to the frontend]{Normalized CPI attributed to the base cycles and stalls caused by the frontend.}
    \label{fig:norm_cpi_frontend}
\end{figure}

In the baseline configuration, all of the benchmarks have a similar number \acrshort{cpi} attributed to \textit{empty ibuffer} stalls ($5-10\%$). These occur when a \acrshort{tb} have no warps in the instruction buffer, but it is not currently blocked in the warp scheduler. Some of the stall cycles are caused by the latency between the warps becoming un-stalled in the warp scheduler and it being fetched, decoded and sent to the ibuffer. For all the benchmarks, excluding \textit{lavaMD, DWT2D, SC} and \textit{CFD}, the number of ibuffer stalls are reduced. As the new frontend is able to cut down on the number of frontend stalls, the number of un-stalls are also reduced, which in turn reduces \textit{empty ibuffer} stalls.

Figure \ref{fig:norm_cpi} shows that \textit{SC} has almost no stalls related to the backend for any of the configurations. That is, when a warp arrives in the ibuffer, it is issued within few cycles. For \textit{SC}, only the \textit{memset} kernel is executed, because of early-exit. The \textit{memset} kernel has almost no data dependencies. Thus the backend is almost never stalling, and the ibuffer is thus being emptied faster than it is filled. The \textit{memset} kernel has a short loop containing a memory write. The loop does require the frontend to stall, due to control flow. There are however no \textit{sync \& control} stalls shown for the \textit{FGTO} and \textit{FRRR} configurations, only \textit{empty ibuffer} stalls. This is because while the warp is stalled in the frontend, all previously fetched warps are issued, hiding the \textit{control} stalls. When the control stall is released, all of the ibuffers are empty, causing \textit{empty ibuffer} stalls. Yet, there are more \textit{empty ibuffer} stalls when using the new frontend than \textit{sync \& control} stalls in the baseline version. This is likely because the icache-stage requires an additional cycle to reorder the icache responses. This results in a somewhat higher latency between a warp being fetched and it arriving in the ibuffer. A similar effect can be seen for other benchmarks such as \textit{srad} and \textit{CFD}. This problem can probably be solved by having a wider frontend, allowing for scheduling more than one instruction per cycle, or have enough warps per \acrshort{sm} to hide the frontend latency.

% memset inner loop
% 80000118: 33 05 c7 00                  	add	a0, a4, a2
% 8000011c: 23 00 b5 00                  	sb	a1, 0(a0)
% 80000120: 13 06 16 00                  	addi	a2, a2, 1
% 80000124: 33 35 f6 00                  	sltu	a0, a2, a5
% 80000128: 6b 00 15 00                  	vx_pred	a0            ; Control stall
% 8000012c: e3 66 f6 fe                  	bltu	a2, a5, -20   ; Control stall

\section{Utilization of Functional Units}

\begin{figure}
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.3\textwidth]{figures/cpi_norm/missed_schedule_wide.png}}
    \caption{Normalized \acrshort{cpi} for benchmarks with most \textit{missed schedule} stalls.}
    \label{fig:cpi_missed_schedule}
\end{figure}

Figure \ref{fig:cpi_missed_schedule} shows the \acrshort{cpi} stacks of the benchmarks with the largest proportion of \textit{missed schedule} stalls. For \textit{psort}, the number of stalls is nearly halved for both \textit{FGTO} and \textit{FRRR}, resulting in a $20\%$ decrease in \acrshort{cpi} from the \textit{\Gls{vortex}} configuration. \textit{Psort} has a low number of frontend stalls, which does not change after implementing the new frontend, as nearly all \textit{sync \& control} stalls are replaced by \textit{empty ibuffer} stalls. \textit{Psort} is thus seeing limited improvements from the frontend changes. The implementation of ready scheduling in \textit{FGTO} and \textit{FRRR} is able to remove all the \textit{missed schedule} stalls. In the \textit{\Gls{vortex}} configuration, \textit{missed schedule} stalls contributes to about $10\%$ of the \acrshort{cpi}, while  \textit{FGTO} the \acrshort{cpi} is decreased by the double ($20\%$). This is because other than frontend and \textit{missed schedule}, \textit{psort} is stalled by data dependencies, which can be hidden by the increased number of issued instructions. Especially \textit{compute data} stalls which have low latency, can be hidden.

The \textit{sgemm} and \textit{nearn} benchmarks also have a significant number of \textit{missed schedule} stalls. In addition to the improvements obtained by ready scheduling, these benchmarks see a great reduction in frontend stalls. The increased throughput of the frontend makes more warps available in the issue stage, increasing the effect of ready scheduling. Because of the significant decrease in frontend stalls, \textit{sgemm} is also able to obtain a $20\%$ reduction in \acrshort{cpi}, similar to \textit{psort}, but with a smaller proportion of \textit{missed schedule} stalls in the \textit{\Gls{vortex}} configuration. The \acrshort{cpi} of \textit{nearn} is not reduced to the same degree as \textit{sgemm} and \textit{psort}. This is likely caused by a combination of multiple factors. The number of frontend stalls are only halved, revealing less additional warps in the ibuffer than \textit{sgemm}. \textit{Nearn} also have less \textit{missed schedule} stalls and more long-latency \textit{memory data} stalls than \textit{psort}, resulting in less stalls being hidden by ready scheduling.

\section{Memory Usage} \label{sec:results_memory}

\begin{figure}
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.3\textwidth]{figures/cpi_norm/exposed_memory.png}}
    \caption{Normalized \acrshort{cpi} stacks for benchmarks where memory stalls are revealed in \textit{FGTO} and \textit{FRRR}}
    \label{fig:exposed_memory}
\end{figure}

% For all benchmarks, most of the stalls in the baseline related to the issue stage are data stalls, i.e. \textit{memory data} and \textit{compute data} stalls. As there are few structural stalls it indicates that the throughput of the functional units are not fully utilized, I.e. \Gls{vortex} is latency bound. If there were more warps or other independent instructions available, they could be issued. With the increased throughput of the frontend, more warps are become available in the instruction buffer. For \textit{hotspot, 3D, lavaMD, backprop, saxpy, vecadd} and \textit{gaussian}, \textit{memory structural} stalls are exposed. This shows that the improvements are able to utilize the throughput of the \acrshortpl{lsu}. The reason why no \textit{compute structural} stalls are exposed is that most compute units have a throughput of one warp per cycle. 


Figure \ref{fig:exposed_memory} shows the normalized \acrshort{cpi} stacks for the benchmarks where \textit{FGTO} and \textit{FRRR} reveal memory stalls. For all of these benchmarks there is a significant number of \textit{memory data} stalls are revealed. Additionally for \textit{hotspot, 3D, lavaMD, saxpy vecadd} and \textit{gaussian}, \textit{memory structural} stalls are also uncovered. As there is an increase in \textit{memory structural} stalls it would be rational to assume an increase in memory bandwidth usage. However the memory bandwidth usage illustrated in Figure \ref{fig:bandwidth_usage_l2}, show the opposite. For the benchmarks where the changes expose \textit{memory structural} stalls, the bandwidth is either decreased or unchanged. Figure \ref{fig:l1_dcache_hitrate} and \ref{fig:l2_dcache_hitrate} show that the dcache hitrates are similar for \textit{\Gls{vortex}}, \textit{FGTO} and \textit{FRRR}. This indicates that there are other causes for the \textit{memory structural} stalls.

\textit{Backprop} and \textit{lavaMD} both have a large number of \textit{memory structural} stalls, but very different bandwidth utilization. \textit{LavaMD} use $17GB/s$ of memory bandwidth on average while \textit{backprop} use $10GB/s$, which is only about half of the available bandwidth. For \textit{lavaMD}, the available bandwidth is likely the main cause of \textit{memory structural} stalls, as its average usage is close to the available bandwidth. This is not the case for \textit{backprop}. Unlike \textit{lavaMD}, \textit{backprop} use a lot of memory barriers. When a memory barrier is issued to the \acrshort{lsu}, all in-flight memory instructions have to complete before the \acrshort{lsu} will be ready. Thus \textit{memory structural} stalls occur while these barriers are resolving. Because of this the average memory bandwidth usage can be low while still producing \textit{memory structural} stalls. This is also why \textit{hotspot} has an increase in \textit{memory structural} stalls, but to a lesser degree, as it is dependant on the latency of the requests and frequency of the barrier instructions.

For all of the benchmarks in Figure \ref{fig:exposed_memory}, a large number of \textit{memory data} stalls are revealed after \textit{FGTO} and \textit{FRRR} are reducing frontend stalls. Because warps are waiting for the results of memory instructions, these stalls are caused by memory latency. It is most likely that these memory requests are already in-flight, and thus the increased frontend throughput just exposes the stall sooner. As there is such a large proportion of \textit{memory data} stalls, it is clear that \Gls{vortex} is incapable of hiding the stalls. There is simply not enough  work to do while waiting for the memory requests. The average latency of memory requests for each benchmark is shown on a logarithmic scale in Figure \ref{fig:avg_mem_latency}. It is a clear trend that \textit{FGTO} and \textit{FRRR} is increasing the memory latency. This could be because of collisions. It is possible that \textit{FRRR} make the memory requests within a shorter time span than \textit{\Gls{vortex}}, resulting in collisions. This is because \textit{FRRR} would have more warps available in the ibuffer, and ready scheduling issues them quicker. Section \ref{sec:results_warp_scheduling} will discuss further why \textit{FGTO} does not perform any better than \textit{FRRR}.

While the dcache hitrate in Figure \ref{fig:dcache_hitrate} stays mostly the same for most benchmarks, \textit{hotspot, sfilter, gaussian} and \textit{NW} see significant increase in L1 hitrate for \textit{FGTO} and \textit{FRRR} over \textit{\Gls{vortex}}. A potential cause of this increase is that \textit{\Gls{vortex}}' find-first warp scheduler is causing some warps to run ahead of others, making them evict cache lines of memory shared between the \acrshortpl{tb}. 

\begin{figure}
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.3\textwidth]{figures/bandwidth/L2.png}}
    \caption[Average bandwidth usage between the L2 cache and main memory.]{Average bandwidth usage between the L2 cache and DDR4 memory}
    \label{fig:bandwidth_usage_l2}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}{\textwidth}
         \centering
         \makebox[\textwidth][c]{\includegraphics[width=1.3\textwidth]{figures/L1_dcache_hitrate/L2.png}}
         \caption{L1 dcache hitrate}
         \label{fig:l1_dcache_hitrate}
     \end{subfigure}
     \hfill
         \begin{subfigure}{\textwidth}
         \centering
         \makebox[\textwidth][c]{\includegraphics[width=1.3\textwidth]{figures/L2_dcache_hitrate/L2.png}}
         \caption{L2 dcache hitrate}
         \label{fig:l2_dcache_hitrate}
    \end{subfigure}
    \caption{Average dcache hitrate}
    \label{fig:dcache_hitrate}
\end{figure}

\begin{figure}
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.3\textwidth]{figures/latency/L2.png}}
    \caption{Average memory latency on logarithmic scale}
    \label{fig:avg_mem_latency}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/instruction_distribution/hotspot3D_unsorted_numbered.png}
    \caption{Distribution of executed instructions per \acrshort{sm} and cluster}
    \label{fig:instruction_distribution_unsorted_3D}
\end{figure}

Figure \ref{fig:instruction_distribution_unsorted_3D} shows the distribution of executed instructions by each of \Gls{vortex}' \acrshortpl{sm} for the \textit{3D} benchmark. It is clear that within each cluster, \textit{\acrshort{sm} 0} is executing close to $5\times$ more instructions than \textit{\acrshort{sm} 7}. For each increasing index within the cluster, the \acrshortpl{sm} are executing less instructions. As there are no idle cycles for \textit{3D}, this cannot be because less work is being allocated to the \acrshortpl{sm}. If the benchmark would have finished instead of exiting early, there would be probably be idle cycles, as the \acrshortpl{sm} with lower indices would finish before the higher indexed \acrshortpl{sm}. The difference in number of executed instructions is likely caused by the \acrshort{noc} distributing the available bandwidth unfairly. A similar effect can be observed for \textit{lavaMD} in figure \ref{fig:instr_dist_lavaMD}, as both \textit{lavaMD} and \textit{3D} exit early and have bandwidth utilization close to the available memory bandwidth.

\textit{SAXPY, vecadd, kmeans} and \textit{gaussian}, see a significant decrease in bandwidth utilization. This decrease is mainly caused by an increase in idle cycles. As some of the cores idle, they stop sending memory requests, lowering the average bandwidth utilization. Section \ref{sec:workload_dist} will explain further why these benchmarks idle.

\section{Workload Distribution} \label{sec:workload_dist}

\begin{figure}
\input{chapters/instruction_distribution}
\caption[Distribution of executed instructions per \acrshort{sm}.]{Distribution of executed instructions per \acrshort{sm} for each of the benchmarks. The \acrshortpl{sm} are sorted in ascending order of their share of the executed instructions}
\label{fig:instruction_distribution}
\end{figure}

\begin{figure}
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.3\textwidth]{figures/cpi_norm/L2_idle.png}}
    \caption{Normalized \acrshort{cpi} stacks for benchmarks with idle cycles}
    \label{fig:norm_cpi_idle}
\end{figure}

\textit{Backprop, saxpy, vecadd, b+tree, kmeans} and \textit{gaussian} all gain a significant number of idle cycles after changing the frontend as shown in Figure \ref{fig:norm_cpi_idle}. All of these benchmarks either finish or complete multiple kernels before being exiting. Thus the effects of uneven memory bandwidth distribution among \acrshortpl{sm} can be observed as idle cycles. As described in Section \ref{sec:results_memory}, the \acrshort{noc} is distributing the available bandwidth unfairly. This makes some \acrshortpl{sm} have higher throughput than others. As the \acrshortpl{tb} are statically distributed at the beginning of the kernel, the throughput difference will cause some of the \acrshortpl{sm} to finish earlier than others. The \acrshortpl{sm} will then have to idle until the kernel has finished execution. This is why the average bandwidth is lower for these benchmarks after implementing the changes. The skew in throughput cannot necessarily be observed in Figure \ref{fig:instruction_distribution} as these benchmarks finish execution which levels out the number of executed instructions by \acrshortpl{sm} idling.

For some of the benchmarks which does not complete before exiting early, \textit{FGTO} and \textit{FRRR} contribute to skewing the memory bandwidth distribution. For \textit{hotspot} (Figure \ref{fig:instr_dist_hotspot}), \textit{DWT2D} (Figure \ref{fig:instr_dist_dwt2d}), \textit{lavaMD} (Figure \ref{fig:instr_dist_lavaMD}) and \textit{NW} (Figure \ref{fig:instr_dist_nw}) the difference between the \acrshortpl{sm} executing the least and most instructions is increased for \textit{FGTO} and \textit{FRRR} compared to \textit{\Gls{vortex}}. This is because \textit{FGTO} and \textit{FRRR} allow the \acrshortpl{sm} to issue more instructions, but the \acrshort{gpu} is throttled by the \acrshort{noc} resulting in small improvements in performance. 

For the benchmarks such as \textit{SC, CFD, HW} and \textit{BFS}, the proportion of idle cycles stays mostly the same for the baseline and new versions. These idle cycles are caused by the \acrshort{tb} scheduler not being able to distribute workload to all of the \acrshortpl{sm}. This is apparent in Figure \ref{fig:instr_dist_bfs}, \ref{fig:instr_dist_cfd}, \ref{fig:instr_dist_hw} and \ref{fig:instr_dist_streamcluster}, where most of the \acrshortpl{sm} are not executing any instructions. This was attempted solved by altering the sizes of local and global work sizes without any luck. A better understanding of the \acrshort{tb} scheduler is probably required to solve this. 

In Figure \ref{fig:norm_cpi_idle}, the \acrshort{cpi} of the \textit{HW} benchmark is almost halved after implementing the new frontend. It appears as if the improvements are a result of a reduction in the number of idle cycles. However, as shown in Figure \ref{fig:instr_dist_hw}, the same number of \acrshortpl{sm} continue to idle, the number of idle cycles thus stays the same. Instead, because of increased frontend throughput and low memory contention, the performance of the active \acrshortpl{sm} improve drastically. The appearance of reduced idle cycles is instead caused by an increase in instructions executed by the active \acrshortpl{sm}, which in turn reduces \acrshort{cpi}.   

\section{Warp scheduling} \label{sec:results_warp_scheduling}

\begin{figure}
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.3\textwidth]{figures/cpi_norm/Schedulers.png}}
    \caption[Normalized \acrshort{cpi} stacks comparing schedulers.]{Comparing combinations of scheduling algorithms used in the issue stage and fetch stage.}
    \label{fig:cpi_norm_schedulers}
\end{figure}

The motivation for implementing new schedulers for \Gls{vortex} was to improve performance over the existing find-first and \acrshort{lrr} algorithms. In this section I will compare combinations of the \acrshort{gto} and \acrshort{lrr} algorithms in the issue and fetch stage. The configurations will be described as \textit{fetch-algorithm}+\textit{issue-algorithm}. All of the configurations discussed in this section implements all the frontend improvements as well as ready scheduling.

For most of the benchmarks shown in Figure \ref{fig:cpi_norm_schedulers}, the scheduling algorithm does not make a large difference. Looking at the average \acrshort{cpi}, the performance seems to be somewhat worse for \textit{RR-GTO}, but the average \acrshort{cpi} stay within $1.5\%$ for all the configurations. The most of the reduction in average performance for \textit{RR-GTO} is coming from \textit{NW}. It is difficult to pinpoint an exact reason for this being an outlier. For \textit{CFD}, the performance is somewhat better when using a round-robin warp scheduler. This is because it has a large number of \textit{empty ibuffer} stalls. A round-robin warp scheduler will be able to fetch warps from different \acrshortpl{tb}, reducing the number of empty ibuffers, and thus reducing \textit{empty ibuffer} stalls. On the other hand, a \acrshort{gto} warp scheduler will attempt to fetch multiple warps from a single \acrshort{tb}. If the warp fetched by \acrshort{gto} cannot be issued, it is less likely to be other warps available in the instruction buffer.

The motivation for using \acrshort{gto} was to improve cache usage and reach long latency stalls earlier. This is clearly not happening, since there is no significant difference in the number of memory stalls between configurations using \acrshort{lrr} and \acrshort{gto} issue schedulers. There are multiple possible reasons why the \acrshort{gto} scheduler might not perform as expected:

\begin{itemize}
    \item The current implementation of \acrshort{gto} does not differentiate between long and short latency stalls. If a warp is stalled due to a short latency stall, e.g a cache hit, its age is set to 0. Thus it will become a low priority warp for the \acrshort{gto} scheduler before it hits a long latency stall. This might also loose potential cache hits, as the \acrshort{gto} scheduler will then begin to schedule another warps which can evict cache-lines used by the short latency stalled warp. Setting the age of the warp to 0 only upon detecting long-latency stalls would allow the warp to keep high priority and hit long-latency stalls sooner. This might improve cache usage and reduce the number of stall cycles due to long-latency memory requests. 
    \item The interaction between the warp scheduler, issue scheduler and \acrshort{bpr} might not be ideal for \acrshort{gto}. To get the most out of \acrshort{gto} scheduling, it should be able to issue consecutively as many warps from the same \acrshort{tb} as possible. If the currently selected \textit{greedy} warp ID of the two schedulers are different, i.e. they schedule warps from different \acrshortpl{tb}, the instruction buffer might empty before hitting the desired long-latency stall. \acrshort{bpr} makes the warp scheduler fetch warps from a different \acrshort{tb} if the instruction buffer is full. This can prevent the warp scheduler from fetching enough instructions from the \acrshort{tb} to hit long-latency stalls. A solution to this would be to increase the queue size of the instruction buffer.  
    \item With only 4 \acrshortpl{tb} per \acrshort{sm}, there might be too few warps to be able to hide any significant number of long-latency stall cycles.
\end{itemize}

% - too few warps to hide stalls in a significant manner
% - bad interaction with the gto warp scheduler and gto issue scheduler
% - short latency stalls stop gto from prioritizing the \acrshort{tb} until it hits a long latency stall 

% \section{Implementation cost}

% \textcolor{red}{Each of the configuration has an associated area cost and critical path which controls the maximum frequency of the GPU. Following is a plot of the total area and critical paths}

\section{Sensitivity Analysis}

I performed various sensitivity analysis based on the memory bandwidth and number of warps per \acrshort{sm} and caches. 

\vspace{1mm}\noindent
\textbf{Increase warps per \acrshort{sm}}. Figure \ref{fig:norm_cpi_8w} shows the \acrshort{cpi} stacks for the same configuration as described in Chapter \ref{chap:exp_setup}, but with 8 warps per \acrshort{sm}. On average \textit{FGTO} and \textit{FRRR} respectively reduce \acrshort{cpi} by $12.9\%$ and $13.2\%$ over \textit{\Gls{vortex}}, which is a greater reduction than when using 4 warps per \acrshort{sm}. \textit{\Gls{vortex}} continue to stall due to lacking frontend throughput. When using 8 warps per \acrshort{sm}, a larger proportion of the stalls are frontend stalls. The increase is mostly due to the increase in \textit{empty ibuffer} stalls. This is because the frontend is unable to throughput enough instructions to fill the increased width of the instruction buffer. The new frontend is clearly still able to fill the instruction buffer, as there is close to no frontend stalls in any of the benchmarks. \textit{SRAD, SC, BFS} and \textit{CFD} continue to experience the same issues as described in Section \ref{sec:result_control_stalls}. 

The increased frontend throughput allows \textit{FGTO} and \textit{FRRR} to utilize memory bandwidth and hide data stalls to a greater degree, as shown by the increased proportion of \textit{memory structural} stalls. The \acrshort{cpi} of \textit{SGEMM} is reduced to a much larger degree by \textit{FGTO} and \textit{FRRR} when using 8 warps than when using 4 warps. When using 4 warps, \textit{SGEMM} is largely dominated by \textit{memory data} stalls. By increasing the number of warps to 8, the low average latency of \textit{sgemm} can be hidden, resulting in a drastic performance improvement. The same goes for psort which is close to never stalling for both \textit{FGTO} and \textit{FRRR}.

The increased \acrshort{mlp} caused by having more warps, is increasing the memory bandwidth requirement. Because of this, the \acrshort{noc}'s skewed bandwidth distribution is likely to have an even greater effect. This is why benchmarks such as \textit{vecadd} is seeing a larger proportion of \textit{idle} stalls. For \textit{gaussian}, the increased proportion of \textit{idle} stalls is likely a combination of skewed bandwidth distribution and the workload not being divided into enough \acrshortpl{tb} to fill the number of slots per \acrshort{sm}.   

% - Figure shows 
% - Baseline has a larger proportion of frontend stalls, as it is unable to schedule instructions to fill the increased width of the instruction buffer
% - The new frontend is able to throughput enough instructions
% - Increase in memory structural stalls and reduction in memory data stalls, this indicates that we are hitting the limit of the memory bandwidth.
% - More idle cycles due to more skew in bandwidth distribution because of more memory contention 

% -sgemm, psort and dwt see a great improvements
%  - psort bound by short latency compute data stalls,
%  - sgemm bound by memory latency, but its latency is lower than others
%  - dwt2d hard to say, something might be wrong 

\begin{figure}
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.3\textwidth]{figures/cpi_norm/L2_8W.png}}
    \caption[Normalized \acrshort{cpi} stacks when using 8 warps per \acrshort{sm}.]{Normalized \acrshort{cpi} stacks for the baseline and final versions using 8 warps per SM}
    \label{fig:norm_cpi_8w}
\end{figure}

% \textcolor{red}{For most of the benchmarks, memory instructions become the largest source of stalls. Note that for all benchmarks except lavaMD, \textit{memory data stalls} are more prevalent than \textit{memory structural stalls}. The \acrshort{lsu} is thus capable of handling more loads. This may indicate that the GPU is latency bound and not bandwidth bound. Increasing the number of warps in each core may introduce more \acrfull{mlp} and thus hide the latency stalls.}

% \textcolor{red}{Doubling the number of warps in each core will alter the number of work items per wave and can thus result in a different number of instructions required to complete the execution of the program. Additionally, the . To ensure that the results are comparable with the results using 4 warps there is no artificial increase in IPC due to sche}

\vspace{1mm}\noindent
\textbf{Increase DDR4 bandwidth.} Figure \ref{fig:2channel_memory} shows the \acrshort{cpi} stacks for \Gls{vortex} using 2 channel DDR4 memory, resulting in $34.8GB/s$ of bandwidth, giving $2\times$ the available bandwidth of the memory configuration described in the experimental setup. For benchmarks with high bandwidth utilization, such as \textit{lavaMD}, the additional bandwidth enables both \textit{FGTO} and \textit{FRRR} to reduce \acrshort{cpi}, while for only 1 channel, the \acrshort{cpi} increased somewhat. For most other benchmarks, the additional memory bandwidth does not alter how the \Gls{vortex} changes affect \acrshort{cpi}. The benchmarks continue to be dominated by \textit{memory data} stalls, and are thus unable to utilize the bandwidth. Thus both \textit{FGTO} and \textit{FRRR} remain latency bound. It is difficult to determine why the \acrshort{cpi} of \textit{NW} and \textit{CFD} for \textit{FGTO} and \textit{FRRR} is increased to almost the double of \textit{\Gls{vortex}}. I believe it might be an issue with the simulation, as I previously experienced issues with the simulation acting up and giving erroneous results, which were fixed by using a different computer for running the simulations.

\begin{figure}
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.3\textwidth]{figures/cpi_norm/L2_2channel_ddr4.png}}
    \caption[Normalized \acrshort{cpi} stacks with double the available memory bandwidth]{Normalized CPI stacks when using two memory channels, giving a bandwidth of $34.8GB/s$}
    \label{fig:2channel_memory}
\end{figure}

\vspace{1mm}\noindent
\textbf{Only L1 caches.} Figure \ref{fig:norm_cpi_L1} shows the normalized \acrshort{cpi} stacks when using only L1 caches, i.e. not using a L2 cache for each cluster. For most of the benchmarks, the increased memory latency of having no L2 cache makes \textit{FGTO} and \textit{FRRR} have less impact on performance as a smaller proportion of the stall cycles can be hidden. Most interestingly, the \textit{idle} stalls incurred by skewed bandwidth distribution in \textit{backprop, saxpy, vecadd, b+tree, kmeans and gaussian} are gone. This indicates that the memory arbiters replacing the L2 caches may be distributing memory bandwidth differently in the \acrshort{noc}. Interestingly \textit{FGTO} and \textit{FRRR} reduce \acrshort{cpi} by $7.02\%$ and $7.42\%$ respectively over \textit{\Gls{vortex}}, which is more than when using an L2 cache. The extra improvement mostly come from there being no additional idle stalls. 

For \textit{NW}, the \acrshort{cpi} is drastically increased for \textit{FGTO} and \textit{FRRR}. Nearly all the the stalls for \textit{NW} are \textit{memory data} stalls. The increased frontend and issue throughput reduces the L1 and L2 dcache hitrate, most likely due to cache contention. Thus by removing the L2 cache, the already high memory latency is increased further for \textit{FGTO} and \textit{FRRR}. As memory latency is already the bottleneck for \textit{NW}, increasing frontend throughput and issue bandwidth hurts performance when using only L1 caches.   

\begin{figure}
    \centering
    \makebox[\textwidth][c]{\includegraphics[width=1.3\textwidth]{figures/cpi_norm/L1.png}}
    \caption{Normalized CPI when using only L1 caches}
    \label{fig:norm_cpi_L1}
\end{figure}