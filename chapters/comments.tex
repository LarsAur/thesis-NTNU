% The following list briefly describes the main observations of the results, before giving a more detailed evaluation in the following sections.
% \begin{enumerate}
%     \item Representing about half of the stalls, \textit{Sync \& control} stalls dominate in the baseline version of \Gls{vortex}. Removing unnecessary frontend stalls, allow the final version to heavily reduce the occurrence of \textit{sync \& control} stalls.
%     \item Compute bound benchmarks see great reductions in \acrshort{cpi} when increasing the throughput of the frontend and implementing ready scheduling, eliminating missed schedules.
%     \item The reduction in \textit{sync \& control} stalls is in most cases revealing more memory stalls.
%     \item Some benchmarks see a significant increase in idle cycles after implementing the changes.
%     \item For some benchmarks \textit{sync \& control} stalls are replaced by \textit{empty ibuffer} stalls.
%     \item For most benchmarks the scheduling algorithm has little to no effect on the performance.

% \end{enumerate}

% However most of the remaining stalls are data dependency stalls and missed schedules. The data stalls are mostly \textit{compute data} stalls, which are low latency, meaning the instructions will be ready in few cycles. By scheduling the ready instructions, and removing missed schedules, the low latency data stalls can be hidden until their operands become available.

% Increasing the throughput of the frontend makes more instructions available in the instruction buffer. This will also increase the number of available warps for the instruction scheduler. This together with a ready scheduler, allows \Gls{vortex} to issue additional instructions as long as they are ready. In the case of \textit{psort}, the number of stalls is halved, resulting in a decrease in \acrshort{cpi} by $20\%$. The number of frontend stalls is low for \textit{psort}. Thus it is seeing limited improvements from the frontend changes. However most of the remaining stalls are data dependency stalls and missed schedules. The data stalls are mostly \textit{compute data} stalls, which are low latency, meaning the instructions will be ready in few cycles. By scheduling the ready instructions, and removing missed schedules, the low latency data stalls can be hidden until their operands become available.


% For \textit{lavaMD} (Figure \ref{fig:instr_dist_lavaMD}), the distribution is skewed. In the baseline version, the highest throughput \acrshort{sm} executed $5.5\times$ more instructions than the \acrshort{sm} with lowest throughput. The distribution gets even worse when implementing the changes, making some of the \acrshortpl{sm} execute close to $0\%$ of the instructions. As LavaMD has close to none \textit{idle} stalls, the discrepancy is clearly not due to inactive \acrshortpl{sm}. Rather it is probable that the \acrshort{noc} is unable to distribute the bandwidth evenly among the \acrshortpl{sm}, resulting in different memory stalls for different \acrshortpl{sm}. The instruction distribution of \textit{backprop} is however even, thus the \acrshort{noc} cannot be at fault for \textit{backprop's} increase in \textit{memory structural} stalls. Unlike lavaMD, backprop use a lot of memory barriers. When a barrier is issued to the \acrshort{lsu}, all in-flight memory instructions have to complete before the \acrshort{lsu} become ready. For the benchmarks with \textit{memory structural} stalls and low bandwidth requirement, barriers are likely to be the cause.  
% This may indicate that the \acrshort{noc} is treating \acrshortpl{sm} unfairly, giving some of the \acrshortpl{sm} significantly less bandwidth than others. The throttled \acrshortpl{sm} would in that case stall due to \textit{memory structural stalls}, while the other \acrshortpl{sm} would be more dominated by \textit{memory data} stalls.

% \textcolor{red}{We observed a set of potential issues with Vortex. The issue scheduler does not have enough information to schedule ready warps. The front-end is unable to provide enough instructions to the issue stage, resulting in less scheduling opportunities.}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.5\textwidth]{example-image-b}
%     \caption{CPI stacks for only baseline}
%     \label{fig:cpi_baseline}
% \end{figure}


% \textcolor{red}{Additionally, we observe that the front-end is unable to bring enough instructions to the issue stage. see \ref{fig:cpi_baseline}}

% \begin{figure}
%     \centering
%     \includegraphics{}
%     \caption{Caption}
%     \label{fig:enter-label}
% \end{figure}

%\textcolor{red}{The baseline issue scheduler checked if the scheduled instruction could be issued after scheduling. This resulted in cycles where warps where not scheduled even tough there ready warps were available}

%\textcolor{red}{This was solved by redesigning the issue stage as shown in \ref{fig:new_issue_stage}}

%\textcolor{red}{For all warps, check if they can issue. Most of the logic is implemented in scoreboard and dispatch. Requires selector in dispatch and scoreboard for each warp to read registers regarding the ready state}

% \textcolor{red}{While Vortex allows for FPGA simulation, it has a main issue, memory bandwidth and latency scaled to the throughput of the GPU. While work is being done at CAL to solve this, I have to continue using software simulation. }

% TODO: Comment on the number of bits used to track gto age and handling of overflow
% TODO: Comment on the size of the ibuffer

% \section{Allocating Memory}

% \textcolor{red}{celenqueuecopybuffer}
% During the porting of the \Gls{rodinia} benchmarks, I observed that\texttt{clEnqueueNDKernels} and \texttt{clCreateBuffer} either aborted or resulted in segmentation faults. Looking into the cause of this crash, I found that \texttt{clEnqueueNDKernels} caused a segmentation fault when the kernel contained local parameters. It seems like the driver is unable to dynamically allocate local memory for the local work groups. To solve this, we hardcoded the local buffers in the kernel scope before compiling the kernels. This introduces a new challenge of selecting the correct buffer size, corresponding to the problem size. Luckily we can just use the size given to \texttt{clSetKernelArg} when setting the local parameters. Still it requires to recompile the kernels when changing input size.

% To improve the insight into Vortex' performance, I expand upon my previous method for generating \textit{cycle-stacks for Vortex}(\acrshort{csv}), giving greater insight into what is causing Vortex to stall. Lastly I broaden Vortex' lacking benchmark suite by implementing benchmarks from Rodinia, a commonly used set of \acrshort{gpu} benchmarks. 

% On average, the frontend improvements reduce the number of frontend stalls by $71\%$. For benchmarks such as \textit{sfilter}, over $99\%$ of the frontend stalls are removed, as all the control stalls are unnecessary and removed. 
% The frontend changes heavily reduce the number of stalls caused by \Gls{vortex}' mechanisms to handle control flow. Together with the new schedulers it allows the issue stage to utilize its functional units to a greater degree. The changes give an average reduction in \acrshort{cpi} of --\% over the baseline, with \textit{psort}, \textit{sgemm} and \textit{Needleman-Wunsch} seeing a 20\% reduction in \acrshort{cpi}. However, some benchmarks see a slight increase or no change to their \acrshort{cpi}. This is due to a lack of memory bandwidth and \acrshort{mlp}, which can be solved by increasing the number of warps per \acrshort{sm}, and use a memory system more suitable for \acrshortpl{gpu}. I observe little to effect of using \acrshort{gto} over \acrshort{lrr}, which is likely due to weaknesses in my implementation. 

%In this thesis, I implement \textit{no-stall-scheduling} and \textit{stall-prediction} allowing vortex to schedule consecutive warps without stalling the frontend, and improve its icache-stage to increase throughput of the fetch stage. Additionally I improve Vortex' schedulers to detect ready warps, removing unnecessary stalls. I also examine the impact of switching from a \textit{loose-round-robin}(\acrshort{lrr}) to a \textit{greedy-then-oldest}(\acrshort{gto}) scheduling algorithm.

% \acrshort{fpga}-akselerasjon fungerer som en middelvei mellom softwaresimulering og maskinvareprototyper, ved å kombinere raske simuleringer med muligheten til å gjøre endringer raskt. Vortex er en RISC-V-basert GPGPU som kan FPGA-akselereres, og kan dermed være en god kandidat for forskning innen \acrshort{gpu} arkitekturer. Tidligere undersøkelser har funnet potensielle flaskehalser i \Gls{vortex}’ frontend og skedulerere. Dette hindret \Gls{vortex} i å utnytte \acrshort{simd} og \acrshort{mlp}, noe som reduserte gjennomstrømningen og gjorde den latensbundet.

% I denne oppgaven implementerer jeg \textit{no-stall-scheduling} og \textit{stall-prediksjon} som gjør det mulig for \Gls{vortex} å skedulere påfølgende warps uten å blokkere dem i frontenden. I tillegg forbedres icache-stadiet ved å øke gjennomstrømmingen av instruksjoner fra instruksjons-cachen. Jeg forbedrer også
% \Gls{vortex}’ skeduler, slik at den kan identifisere warps som er klare, dette fjerner unødvendige ventesykler. Jeg undersøker også virkningen av å bytte fra en \textit{loose-round-robin} (LRR) til en \textit{greedy-then-oldest} (GTO) algoritme for skedulering.

% For å forbedre innsikten i Vortex’ ytelse, utvider jeg mine tidligere
% implementasjon for å generere \textit{cycle-stacks for Vortex} (CSV), for å gi innblikk i
% hva som får Vortex til å sakke ned. Til slutt utvider jeg Vortex' manglende benchmark-suite
% ved å implementere benchmarks fra Rodinia, et ofte brukt sett med GPU-benchmarks.

% Endringene i frontenden reduserer antallet ventesykler forårsaket av \Gls{vortex}'
% mekanismer for å håndtere kontrollflyt. Sammen med de nye skedulererene muliggjør det for
% issue-stadiet å utnytte de funksjonelle enhetene i større grad. Forandringene
% gir en gjennomsnittlig reduksjon i \acrshort{cpi} på –\% over utgangspunktet, hvor \textit{psort, sgemm} og
% \textit{Needleman-Wunsch} ser en reduksjon i \acrshort{cpi} på $20\%$. Noen benchmarks
% opplever en liten økning eller ingen endring i \acrshort{cpi}. Dette skyldes en mangel på minne
% båndbredde og minnenivå parallellitet, som kan løses ved å øke antall warps per
% \acrshort{sm}, og bruk et minnesystem som er mer egnet for GPUer. Jeg observerer få forskjeller i bruk av
% GTO over LRR, noe som sannsynligvis skyldes svakheter i implementasjonen min.

%  They implemented a 3D graphics rendering accelerator supporting the Vulkan API, a modern graphics rendering API compared to the commonly used older OpenGL API. They also introduce a \Gls{riscv} \acrshort{isa} for accelerating graphics rendering, as well as implementing a compiler-driven control-flow divergence transformation to handle rendering loops inside graphics kernels. While \Gls{vortex} provides hardware texture units, Skybox has a hardware rasterizer and \acrfull{rop}. This results in a GPU more suitable for graphics workloads than \Gls{vortex}, as the \Gls{vortex} GPGPU is mostly suited for compute workloads.

% \Acrfullpl{gpu} are accelerators designed for executing highly parallelizable workloads. They are able to achieve high throughput by exploiting \acrfull{simt}. GPUs have a number of \acrfullpl{sm}, each with a set of parallel execution lanes, which can execute a set of logically independent threads. These threads are all a part of the same program known as the kernel. The programming model of GPUs is \textit{data-parallel}, that is the program is divided into a set of threads, all executing the same program, but with different data. As illustrated in Figure \ref{fig:kernel_work_items}, the kernel is divided into threads which is grouped into blocks called \acrfullpl{tb}. These \acrshortpl{tb} are then allocated and executed on the \acrshortpl{sm} in the \acrshort{gpu}. The \acrshortpl{sm} can be allocated a number of \acrshortpl{tb} at the same time. During execution, each \acrshort{sm} can execute \textit{warps} from all of its allocated \acrshortpl{tb}. A \textit{warp} is the next instruction in a \acrshort{tb} and is the basic execution unit of the \acrshort{gpu}. The threads within a warp run in lock-step, this means that all the threads execute the same instruction. In the case of divergence between threads, the branching threads are disabled, while the non-branching threads continue execution until their paths rejoin.

% For benchmarks with multiple kernels, the start-up time of each kernel become a relevant if the benchmark is not exited by early part of the performance

% Software simulation is much slower than running on ASICs or FPGAs. To keep the simulation time reasonable, I implement fast-forwarding, warm-up and early termination. \textit{Early termination} is implemented such that when \Gls{vortex} has ran for a given number of cycles, it jumps to the exit routine and dumps all performance metrics. This allows us to run benchmarks with realistic working-sets, to obtain realistic performance metrics. It is not ideal to terminate early, as the full behaviour of the benchmarks might not be captured. Additionally will changes to the architecture change the number of instructions executed within the simulated cycles. This is still a common solution \cite{simpoint}, as there are not many other options. Due to the increased number of benchmarks it is still likely that the results will be representable of \Gls{vortex}' performance.  

% It would be ideal to terminate based on the number of instructions committed, such that benchmark behaviour would be independent of changes to performance. However, as each core terminate based on internal metrics, cores with idle warps, would commit less instructions, using a disproportionate number of cycles compared to a core full of active warps. Thus I decided to terminate early based on cycle count.

% \textit{Fast-forwarding} is done by not collecting performance metrics in the start of the simulation. When doing early termination, the start of the simulation (e.g starting warps) represents a larger proportion of the total run-time. Not including the start-up, is thus important to obtain performance metrics representing the majority of the runtime. \textit{Warm-up} is removing the cold-start bias occurring at the beginning of the benchmark. This is due to empty caches, branch buffers, pre-fetchers, etc. By extending the number of cycles fast-forwarded, the GPU can be warmed up to give a more stable performance when simulating less cycles.

% Initially the L1 and L2 cache hit-rates were collected for a subset of the benchmarks. These are plotted in \ref{fig:l1_cache_hitrate} and \ref{fig:l2_cache_hitrate} for L1 and L2 cache respectively. After 30000 cycles, the cache hit-rate is stabilizing for both the L1 and L2 cache. While the L1 hit-rate for backprop is fluctuating, this is probably due to its access pattern, as similar fluctuations show up. To give some extra margins, 50000 was selected as the number of cycles to fast-forward.

% For benchmarks with multiple kernels, the start-up time of each kernel become a relevant if the benchmark is not exited by early part of the performance...