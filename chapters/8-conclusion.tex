\chapter{Conclusion and Further Work}

\section{Conclusion}

In this thesis, I proposed and implemented improvements to \Gls{vortex}' frontend, increasing the bandwidth of the fetch, decode and issue stages. The proposed changes were based on the work and findings in my project thesis. This accounts for contribution \textbf{C1} corresponding with task \textbf{T1}.

I improved \acrshort{csv} by making it consider the stall cause of all warps in the issue stage. This gives a better overview of the issue stage and enables it to show frontend stalls for individual warps. This improvement comprises contribution \textbf{C2} which corresponds with task \textbf{T2}. 

Through evaluating the implemented changes, I find that the increased frontend throughput and issue bandwidth has varying effects on the performance. The improvements to the frontend, reduce frontend related stalls by $71\%$ on average, and even remove all frontend stalls from \textit{sfilter}, as it has no control-flow instructions. The improved ready instruction scheduler removes all missed scheduling opportunities. This is reducing the \acrshort{cpi} of \textit{psort} by $20\%$, as low-latency stalls can be hidden. However, the \acrshort{cpi} is only reduced by $5.4\%$. This is because the bottleneck is moved to the backend of \Gls{vortex}, as warps have to wait for long-latency memory stalls. I find that the uncovered \textit{memory data} stalls are a consequence of the \Gls{vortex} configuration being too small. This is also why there is no significant performance gain when using \acrshort{gto} instead of \acrshort{lrr} scheduling. This evaluation constitutes contribution \textbf{C3} corresponding with task \textbf{T3}.

I adapt Rodinia benchmarks to enable them to run on \Gls{vortex}. This is improving the analysis, by adding a set of benchmarks more representative of real workloads. By additionally enabling \Gls{vortex} to profile multi-kernel programs, it becomes possible to add a wider range of benchmarks. This comprises contribution \textbf{C4} which corresponds with task \textbf{T4}.

\section{Further Work}

Though the improvements I proposed in this thesis improved frontend and issue bandwidth, they did not improve the performance to the same degree. The reason behind this is most likely that the \acrshortpl{sm} are too small to hide latency. Having more warps per \acrshort{sm} together with a more fitting memory system is likely required to solve this problem. However, it is infeasible to continue using software simulation while increasing the size of the \acrshortpl{sm} and the \acrshort{gpu}. To solve this \acrshort{fpga}-acceleration is required. Thus the natural next step is to integrate \Gls{vortex} into Chipyard and FireSim to resolve the issues regarding the discrepancies between \acrshort{fpga} and \acrshort{asic} clock frequencies.

There are also other issues which need to be resolved to make \Gls{vortex} a reasonable \acrshort{gpu}. \Gls{vortex} is unable to perform efficient workload balancing. In some cases, the \acrshort{tb} scheduler is unable to distribute work to all of the \acrshortpl{sm}, making them idle. In other cases, \acrshortpl{sm} finish execution long before others. Gaining a \text{better} understanding of \Gls{vortex}' \acrshort{tb} scheduler and workload distribution is probably required to understand the source of the problem. It is likely better to implement a dynamic system, similar to the one made by Nvidia~\cite{CTA_scheduling}.  