\chapter{Background}

\section{GPU Overview}

\Acrfullpl{gpu} are designed for executing highly parallelizable workloads. They achieve high throughput by exploiting \acrshort{simt}. This section will first cover the \acrshort{gpu} programming model, before describing the general architecture of a \acrshort{gpu}. 

\subsection{GPU Programming Model}

% Source: https://cims.nyu.edu/~schlacht/OpenCLModel.pdf

OpenCL is a commonly used programming framework for heterogeneous parallel computing for cross-vendor and cross-platform hardware. As \Gls{vortex} supports OpenCL, I will refer to the OpenCL programming model~\cite{opencl_introduction}, although the concepts described are transferable to CUDA. The program running on the \acrshort{gpu} is known as the kernel. The kernel is a function which can be executed in parallel over a predefined number of dimensions. The kernel is divided into a set of \textit{threads}, also referred to as \textit{work-items}. As illustrated in Figure~\ref{fig:kernel_work_items}, these threads are grouped into \textit{\acrlongpl{tb}} (\acrshortpl{tb}) which are also known as \textit{work-groups} or \textit{cooperative thread arrays} (CTAs).

All the threads execute the same kernel function in an N-dimensional domain over a region of memory, i.e. the threads execute the same instructions on different data, based on their thread and \acrshort{tb} index. The size and number of dimensions of the \acrshortpl{tb} are determined by the \textit{local work sizes}. The dimensions of the kernel grid and the number of \acrshortpl{tb} are then given by the \textit{global work sizes} divided by the number of threads per \acrshort{tb}. Both the \textit{local} and \textit{global work sizes} are given by the application when executing the kernel on the GPU.

After defining the grouping of the threads into \acrshortpl{tb}, each \acrshort{tb} will be executed concurrently within a compute unit. All the threads within the \acrshort{tb} will execute in lockstep, i.e. they will all execute the same instruction at the same time. An instruction executed by all the threads in a \acrshort{tb} is known as a \textit{warp}. If the kernel contains branches, the threads within a \acrshort{tb} might diverge and have to execute different execution paths. As the threads are executed in lockstep, one execution path has to be executed before the other. When executing branching paths, threads are masked out to make only the correct threads execute the instructions. How this is handled varies based on the architecture of the \acrshort{gpu}.

% Memory model

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/grid.png}
    \caption[Relation between the kernel, thread blocks, threads and warps]{Illustration of how the kernel is divided into a grid of thread blocks with threads running in lockstep. The number of threads in the kernel is determined by the global work sizes, and the number of threads in each thread block is set by the local work sizes}
    \label{fig:kernel_work_items}
\end{figure} 

\subsection{GPU Architecture}

A high-level block diagram of a \acrshort{gpu} is shown in Figure~\ref{fig:generic-gpu}. To achieve high throughput for highly parallel workloads, \acrshortpl{gpu} dedicates a large number of its transistors to computation. \acrshortpl{gpu} have a number of \textit{\acrlongpl{sm}}~(\acrshortpl{sm}) all having a set of parallel execution cores. The \acrshortpl{sm} can execute a set of logically independent threads by executing each thread on an execution core. The threads do however run in lockstep, as the control logic is shared among the execution cores. 

Typically each \acrshort{sm} have a dedicated L1 cache, and share an L2 cache with the other \acrshortpl{sm} in the same cluster. These caches are often smaller than CPU caches as \acrshortpl{gpu} are less latency sensitive. By having multiple \acrshortpl{tb} allocated to each \acrshort{sm}, the \acrshort{gpu} can mask stalls by interleaving their warps. As \acrshortpl{sm} have multiple \acrfullpl{fu} to execute different instructions, it is likely that if a warp stalls, a warp from another \acrshort{tb} can be executed. This results in a high degree of \acrfull{tlp} and \acrfull{mlp}. Due to the amount of \acrshort{mlp}, \acrshortpl{gpu} require high memory bandwidth~\cite{get_out_of_the_valley}. The \acrshort{noc} and memory system have to handle this high bandwidth requirement, which is why \acrshortpl{gpu} typically use specialized memory types such as GDDR or HBM. 

When a kernel is executed on a \acrshort{gpu}, the \acrshortpl{tb} have to be divided among the \acrshortpl{sm}. This is the job of the \acrshort{tb} scheduler. The \acrshort{tb} scheduler attempts to balance the workload evenly among the \acrshortpl{sm} during execution. Figure~\ref{fig:tb_scheduler} shows an example of how a \acrshort{tb} scheduler might distribute \acrshortpl{tb} among \acrshortpl{sm}. For Nvidia \acrshortpl{gpu}, it attempts to maximize \acrshort{tb} occupancy~\cite{CTA_scheduling}, i.e maximize the number of \acrshortpl{tb} in \acrshortpl{sm} at all times. This is done by periodically obtaining information from every \acrshort{sm} regarding the available resources over a dedicated network, and selecting the \acrshort{sm} best fit for the next \acrshort{tb}. The \acrshort{tb} scheduler can account for factors such as data locality when selecting the best fitting \acrshort{sm}. In the case of clustered \acrshortpl{sm}, the \acrshort{tb} scheduler can also aim at distributing \acrshortpl{tb} evenly among the clusters, or map close \acrshortpl{tb} to the same cluster~\cite{wang2020modeling}. Having good load balancing will allow the \acrshortpl{sm} to execute the kernel efficiently and balance the load evenly to obtain high utilization of the \acrshortpl{sm} and reduce idling.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/generic-gpu.png}
    \caption{High-level block diagram of a GPU}
    \label{fig:generic-gpu}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/TB_scheduler.png}
    \caption[\Acrlong{tb} scheduling]{Thread block scheduler scheduling \acrshortpl{tb} in a round-robin order among the \acrshortpl{sm} in the same cluster, but attempting to map neighbouring blocks to the same cluster to exploit locality.}
    \label{fig:tb_scheduler}
\end{figure}

\newpage
\section{Warp Scheduling} \label{sec:warp_scheduling}

The warp scheduler is a part of the \acrshort{sm}'s control logic, it selects the next warp to be executed. It does this by selecting one of the allocated \acrshortpl{tb} and scheduling its next warp. The scheduling algorithm used by the warp scheduler can be integral to the performance of the GPU. Two commonly used warp scheduling algorithms are \acrfull{lrr} and \acrfull{gto}~\cite{improving_gpgpu_scheduling}. Figure~\ref{fig:lrr} and \ref{fig:gto} respectively illustrate how \acrshort{lrr} and \acrshort{gto} schedules warps. \acrshort{lrr} schedules warps in a round-robin order. If a warp is stalled, it is skipped, and the next warp can be scheduled. \acrshort{gto} selects the \acrshort{tb} with the oldest ready warp and schedules warps from the \acrshort{tb} until it stalls~\cite{cache-conscious_wavefront_scheduling}. 

In the case of memory intensive applications, \acrshort{lrr} can cause a situation where all warps arrive at long latency stalls at the same time~\cite{ZHANG2018520}, as shown in Figure~\ref{fig:lrr-lls}. If all warps are stalled, the long latency stalls cannot be hidden, resulting in low throughput. The goal of the \acrshort{gto} scheduling algorithm is to reach a stall for a single warp before scheduling other warps. This may enable more stalls to be hidden, as shown in Figure~\ref{fig:gto-lls}, and even achieve better cache locality~\cite{cache-conscious_wavefront_scheduling, ZHANG2018520}.

% Other notable scheduling algorithms are:
There are also other notable warp schedulers which can give somewhat better performance than \acrshort{gto}, but also require more information about the state of the \acrshort{gpu} and the warps.

\textit{Cache-conscious wavefront scheduling} (CCWS)~\cite{cache-conscious_wavefront_scheduling} attempts to dynamically determine how many, and which warps should be allowed to access, using feedback from the L1 cache. The goal of CAWS is to reduce the number of memory accesses between re-references by the same \acrshort{tb}, to keep cache hit rates high.

\textit{Criticality-aware warp scheduling} (CAWS)~\cite{caws} attempts to improve the execution time of critical warps, i.e. warps which require more time and resources to complete. By doing this, the execution time disparity between the \acrshortpl{tb} is equalized, reducing the total execution time.

\textit{Lazy warp scheduling} (LWS)~\cite{improving_gpgpu_scheduling} attempts to schedule low-latency warps first, such that they can be masked by scheduling longer latency warps afterwards. By prioritizing low-latency warps, there are fewer stall cycles.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/warp-scheduling-lrr-2.png}
         \caption{Loose round-robin}
         \label{fig:lrr}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/warp-scheduling-gto-2.png}
         \caption{Greedy then oldest}
         \label{fig:gto}
     \end{subfigure}
    \caption[Demonstration of how LRR and GTO selects warps for scheduling]{Demonstration of how LRR and GTO selects warps for scheduling. For GTO, the ages of the ready warps are written in the cells}
    \label{fig:lrr_gto}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/warp-scheduling-lrr-stall-2.png}
         \caption{Loose round-robin}
         \label{fig:lrr-lls}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/warp-scheduling-gto-stall-2.png}
         \caption{Greedy then oldest}
         \label{fig:gto-lls}
     \end{subfigure}
        \caption[Demonstration of how \acrshort{lrr} and \acrshort{gto} handles long-latency stalls]{Demonstration of how \acrshort{lrr} and \acrshort{gto} performs in conjunction with long-latency stalls. \acrshort{gto} is able to hide the stalls, while \acrshort{lrr} is unable to because all warps stall at the same time}
        \label{fig:lrr-gto-lls}
\end{figure}

\section{GPU Simulation} \label{sec:gpu_simulation}

GPGPU architecture research is mainly focused on using software simulations~\cite{gem5_gpu, gpu_sim_cuda, multi2sim} modeling the architecture at the \acrfull{il} level, such as PTX and HSAL. There are however significant deviations between GPU simulators using high-level abstractions and real hardware. Simulating IL instructions can add up to 33\% error when comparing the absolute runtimes to real hardware~\cite{lost_in_abstraction}. High-level abstraction models have substantially less functional state associated with the instructions. Thus they are unable to model important micro-architectural interactions, such as instruction fetching and control flow divergence.

To obtain results which most accurately reflect the performance of an architecture, cycle-accurate simulations are required. A solution to the inaccurate high-level abstraction simulations is \acrfull{rtl} implementations. These implementations are cycle-accurate, but require substantially more time and memory to simulate. This is because the entire state of the system is represented. There exist several \acrshort{rtl} implementations of open-source GPGPUs, such as MIAOW~\cite{MIAOW}, Nyami~\cite{Nyami} and FlexGrip~\cite{FlexGrip}. However, the \acrshortpl{isa} used in these \acrshortpl{gpu} are either custom or proprietary, which restricts application support. \Gls{vortex} solves this by basing its \acrshort{isa} on the open source \Gls{riscv} \acrshort{isa} and including a custom compiler. By additionally having OpenCL support, adapting applications for \Gls{vortex} becomes easier.

In addition to creating \Gls{vortex}, the team at Georgia Tech presented Skybox~\cite{skybox} at ASPLOS 2023. Similarly to \Gls{vortex}, Skybox can be \acrshort{fpga}-accelerated, but is more focused on rendering graphics. It has support for Vulkan, a modern graphics rendering API, and it has a hardware rasterizer and \acrfull{rop}. This results in a GPU more suitable for graphics workloads than \Gls{vortex}, as the \Gls{vortex} GPGPU is mostly suited for compute workloads.

Simulating entire systems in software is rather slow, especially for large parallel systems as the simulations are difficult to parallelize due to fine-grained synchronization~\cite{graphite, wwt2}. To speed up architecture simulation, \acrshort{fpga}-acceleration can be used. RAMP-gold~\cite{RAMP-gold}, an FPGA multicore simulator, achieved a $263\times$ speedup over GEMS~\cite{gems}, a software-based simulator. \acrshort{fpga}-acceleration serves as a middle ground between software simulation and \acrshortpl{asic}. As high-end \acrshortpl{fpga} are becoming more prevalent in the consumer market, implementing full-feature \acrshortpl{gpgpu} is becoming a possibility.

There is however a critical problem when running \acrshort{fpga}-accelerated simulations, modeling the timing and behaviour of I/O and peripherals~\cite{chipyard}, e.g. DRAM. To obtain representative results using \acrshort{fpga}-accelerated GPUs, both the memory bandwidth and the latency needs to be scaled to match the discrepancy between \acrshortpl{fpga} and \acrshortpl{asic}. Chipyard~\cite{chipyard} achieves this using Firesim~\cite{firesim}. Firesim, use a token mechanism which can stall individual SoCs of the simulated system to advance the system in target time. \Gls{vortex} does not have any mechanisms to solve this problem, however, work is currently being done at NTNU's \text{\acrfull{cal}} to implement \Gls{vortex} into Chipyard. Meanwhile, I have to simulate \Gls{vortex} and DRAM in software to obtain representative results.

% Presentation by vortex group 
% https://www.youtube.com/watch?v=h1xDQILSZnI&ab_channel=MICROSymposium

