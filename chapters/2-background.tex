\chapter{Background}

\section{GPU Overview}

\Acrfullpl{gpu} are designed for executing highly parallelizable workloads. They achieve high throughput by exploiting \acrshort{simt}. This section will first cover the \acrshort{gpu} programming model, before describing the general architecture of a \acrshort{gpu}. 

\subsection{GPU Programming Model}

% Source: https://cims.nyu.edu/~schlacht/OpenCLModel.pdf

OpenCL is a commonly used programming framework for heterogeneous parallel computing for cross-vendor and cross-platform hardware. As \Gls{vortex} supports OpenCL, I will refer to the OpenCL programming model~\cite{opencl_introduction}, although the concepts described are transferable to CUDA. The program running on the \acrshort{gpu} is known as the kernel. The kernel is a function which can be executed in parallel over a predefined number of dimensions. The kernel is divided into a set of \textit{threads}, also referred to as \textit{work-items}, as illustrated in Figure~\ref{fig:kernel_work_items}. These threads are grouped into \textit{\acrlongpl{tb}} (\acrshortpl{tb}) which are also known as \textit{work-groups} or \textit{cooperative thread arrays} (CTAs).

All the threads execute the same kernel function in an N-dimensional domain over a region of memory, i.e. the threads execute the same instructions on different data, based on their thread and \acrshort{tb} index. The size and number of dimensions of the \acrshortpl{tb} are determined by the \textit{local work sizes}. The dimensions of the kernel grid and the number of \acrshortpl{tb} are then given by the \textit{global work sizes} divided by the number of threads per \acrshort{tb}. Both the \textit{local} and \textit{global work sizes} are given by the application when queueing the kernel on the GPU.

After defining the grouping of the threads into \acrshortpl{tb}, each \acrshort{tb} will be executed concurrently within a compute unit. All the threads within the \acrshort{tb} will execute in lockstep, i.e. they will all execute the same instruction at the same time. An instruction executed by all the threads in the \acrshort{tb} is known as a \textit{warp}. If the kernel contains branches, the threads within a \acrshort{tb} might diverge and have to execute different execution paths. As the threads are executing in lockstep, one execution path have to be executed before the other. When executing branching paths, threads are masked out to make only the correct threads execute the instructions. How this is handled varies based on the architecture of the \acrshort{gpu}.

% Memory model

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figures/grid.png}
    \caption[Relating the kernel grid, warps and threads]{Illustration of how the kernel is divided into a grid of thread blocks, all containing a set number of threads running in lockstep. The threads all execute the same instructions, but on different data.}
    \label{fig:kernel_work_items}
\end{figure} 

\subsection{GPU Architecture}

A high-level block diagram of a \acrshort{gpu} is shown in Figure~\ref{fig:generic-gpu}. To achieve high throughput for highly parallel workloads, \acrshortpl{gpu} dedicate a large number of its transistors to computation. \acrshortpl{gpu} have a number of \textit{\acrlongpl{sm}}(\acrshortpl{sm}) all having a set of execution cores. The \acrshortpl{sm} can execute a set of logically independent threads by executing each thread on an execution core. The threads does however run in lockstep, as the control logic is shared among the execution cores. 

Typically each \acrshort{sm} have a dedicated L1 cache, and share an L2 cache with other \acrshortpl{sm} in the same cluster. These caches are often smaller than CPU caches as \acrshortpl{gpu} are less latency sensitive. By having multiple \acrshortpl{tb} allocated to each \acrshort{sm}, the \acrshort{gpu} can mask stalls by interleaving their warps. As \acrshortpl{sm} have multiple \acrfullpl{fu} to execute different instructions, it is likely that if a warp stalls a warp from another \acrshort{tb} can be executed. This results in a high degree of \acrfull{tlp} and \acrfull{mlp}. Due to the amount of \acrshort{mlp}, \acrshortpl{gpu} require high memory bandwidth~\cite{get_out_of_the_valley}. The \acrshort{noc} and memory system have to handle this high bandwidth requirement, which is why \acrshortpl{gpu} typically use specialized memory types such as GDDR or HBM. 

When a kernel is executed on a \acrshort{gpu}, the \acrshortpl{tb} have to be divided among the \acrshortpl{sm}. This is the job of the \acrshort{tb} scheduler. The \acrshort{tb} scheduler attempts to balance the workload evenly among the \acrshortpl{sm} during execution. Figure~\ref{fig:tb_scheduler} shows an example of how a \acrshort{tb} scheduler might distribute \acrshortpl{tb} among \acrshortpl{sm}. For Nvidia \acrshortpl{gpu}, it attempts to maximize \acrshort{tb} occupancy~\cite{CTA_scheduling}, i.e maximize the number of \acrshortpl{tb} in \acrshortpl{sm} at all times. This is done by periodically obtaining information from every \acrshort{sm} regarding the available resources over a dedicated network, and selecting the \acrshort{sm} best fit for the next \acrshort{tb}. The \acrshort{tb} scheduler can account for factors such as data locality when selecting the best fitting \acrshort{sm}. In the case of clustered \acrshortpl{sm}, the \acrshort{tb} scheduler can also aim at distributing \acrshortpl{tb} evenly among the clusters, or map close \acrshortpl{tb} to the same cluster~\cite{wang2020modeling}. Having good load balancing will allow the \acrshortpl{sm} to execute the kernel efficiently and balance the load evenly to obtain high utilization of the \acrshortpl{sm} and reduce idling.

\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/generic-gpu.png}
    \caption{High-level block diagram of a GPU}
    \label{fig:generic-gpu}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/TB_scheduler.png}
    \caption[\acrshort{tb} scheduling]{Thread block scheduler scheduling \acrshortpl{tb} in a round-robin order among the \acrshortpl{sm} in the same cluster, but attempting to map neighbouring blocks to the same cluster to exploit locality.}
    \label{fig:tb_scheduler}
\end{figure}

\section{GPU Simulation} \label{sec:gpu_simulation}

GPGPU architecture research is mainly focused on using software simulations~\cite{gem5_gpu, gpu_sim_cuda, multi2sim} modeling the architecture at the \acrfull{il} level such as PTX and HSAL. There are however significant deviations between GPU simulators using high-level abstractions and real hardware. Simulating IL instructions can add up to 33\% error when comparing the absolute runtimes to real hardware~\cite{lost_in_abstraction}. High-level abstraction models have substantially less functional state associated with the instructions. Thus they are unable to model important micro-architectural interactions, such as instruction fetching and control divergence.

To obtain results reflecting the performance of an architecture most accurately, cycle accurate simulations are required. A solution to the inaccurate high-level abstraction simulations, is \acrfull{rtl} implementations. These implementations are cycle accurate, but require substantially more time and memory to simulate. This is because the entire state of the system is represented. There exists several \acrshort{rtl} implementations of open-source GPGPUs, such as MIAOW~\cite{MIAOW}, Nyami~\cite{Nyami} and FlexGrip~\cite{FlexGrip}. However, the \acrshortpl{isa} used in these \acrshortpl{gpu} are either custom or proprietary, restricting application support.

In addition to creating \Gls{vortex}, the team at Georgia Tech presented Skybox~\cite{skybox} at ASPLOS 2023. Skybox can also be \Gls{fpga}-accelerated, and is focused on rendering graphics. It has support for Vulkan, a modern graphics rendering API, and it has a hardware rasterizer and \acrfull{rop}. This results in a GPU more suitable for graphics workloads than \Gls{vortex}, as the \Gls{vortex} GPGPU is mostly suited for compute workloads  

Simulating entire systems in software is rather slow, especially for large multi-core systems as they are difficult to parallelize due to fine-grained synchronization~\cite{graphite, wwt2}. To speed up architecture simulation, \acrshort{fpga}-acceleration can be used. RAMP-gold~\cite{RAMP-gold}, an FPGA multicore simulator, achieved a $263\times$ speedup over GEMS~\cite{gems}, a software based simulator. \acrshort{fpga}-acceleration serves as a middle ground between software simulation and \acrshortpl{asic}. As high-end \acrshortpl{fpga} are becoming more prevalent in the consumer market, implementing full-feature \acrshortpl{gpgpu} is becoming a possibility.

There is however a critical problem when running FPGA-accelerated simulations, modeling the timing and behaviour of I/O and peripherals~\cite{chipyard}, e.g DRAM. To obtain representative results using \acrshort{fpga}-accelerated GPUs, both memory bandwidth and latency needs to be scaled to match the discrepancy between \acrshortpl{fpga} and \acrshortpl{asic}. Chipyard~\cite{chipyard} is able to achieve this using Firesim~\cite{firesim}. Firesim, use a token mechanism which can stall individual SoCs of the simulated system to advance the system in target time. Vortex does not have any mechanisms to solve this problem, however work is currently being done at NTNU's \acrfull{cal} to implement \Gls{vortex} into Chipyard. Meanwhile, I have to simulate \Gls{vortex} and DRAM in software to obtain representative results.

\section{Warp Scheduling} \label{sec:warp_scheduling}

The warp scheduler and the scheduling algorithm can be integral to the performance of the GPU. Two commonly used warp scheduling algorithms are \acrfull{lrr} and \acrfull{gto}~\cite{improving_gpgpu_scheduling}. Figure~\ref{fig:lrr} and \ref{fig:gto} respectively illustrates how \acrshort{lrr} and \acrshort{gto} schedules warps. \acrshort{lrr} schedules warps in a round-robin order. If a warp is stalled, it is skipped, and the next warp can be scheduled. \acrshort{gto} selects the \acrshort{tb} with the oldest ready warp and schedules warps from the \acrshort{tb} until it stalls~\cite{cache-conscious_wavefront_scheduling}. 

In the case of memory intensive applications, \acrshort{lrr} can cause situation where all warps arrive at long latency stalls at the same time~\cite{ZHANG2018520}, as shown in Figure~\ref{fig:lrr-lls}. If all warps are stalled, the long latency stalls cannot be hidden, resulting in low throughput. Scheduling algorithms such as \acrshort{gto}, aims at reaching a stall for a single warp before scheduling others. This may enable more stalls to be hidden, as shown in Figure~\ref{fig:gto-lls}, and even achieve better cache locality~\cite{cache-conscious_wavefront_scheduling, ZHANG2018520}.

% Other notable scheduling algorithms are:
There are also other notable warp schedulers which can give somewhat better performance gains than \acrshort{gto}, but also require more information about the state of the \acrshort{gpu} and the warps.

\textit{Cache-conscious wavefront scheduling} (CCWS)~\cite{cache-conscious_wavefront_scheduling} attempts to dynamically determine how many, and which warps should be allowed to access, using feedback from the L1 cache. The goal of CAWS is to reduce the number of memory accesses between re-references by the same \acrshort{tb}, to keep cache hit rates high.

\textit{Criticality-aware warp scheduling} (CAWS)~\cite{caws} attempts to improve the execution time of critical warps, i.e. warps which require more time and resources to complete. By doing this, the execution time disparity between the \acrshortpl{tb} is equalized, reducing the total execution time.

\textit{Lazy warp scheduling} (LWS)~\cite{improving_gpgpu_scheduling} attempts to schedule short latency warps first, such that they can be masked by scheduling longer latency warps afterwards. By prioritizing short latency warps, there are less stall cycles.

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/warp-scheduling-lrr-2.png}
         \caption{Loose round-robin}
         \label{fig:lrr}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/warp-scheduling-gto-2.png}
         \caption{Greedy then oldest}
         \label{fig:gto}
     \end{subfigure}
    \caption{Demonstration of how LRR and GTO selects warps for scheduling. For GTO, the age of the ready warps are written in the cells}
    \label{fig:lrr_gto}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/warp-scheduling-lrr-stall-2.png}
         \caption{Loose round-robin}
         \label{fig:lrr-lls}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/warp-scheduling-gto-stall-2.png}
         \caption{Greedy then oldest}
         \label{fig:gto-lls}
     \end{subfigure}
        \caption[Demonstration of how \acrshort{lrr} and \acrshort{gto} handles long-latency stalls]{Demonstration of how \acrshort{lrr} and \acrshort{gto} performs in conjunction with long-latency stalls. \acrshort{gto} is able to hide the stalls, while \acrshort{lrr} is unable to because all warps stall at the same time}
        \label{fig:lrr-gto-lls}
\end{figure}

% Presentation by vortex group 
% https://www.youtube.com/watch?v=h1xDQILSZnI&ab_channel=MICROSymposium

